{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e74a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11dedce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries= gpd.read_file(r\"data\\countries.geojson\")\n",
    "clean_elections=pd.read_csv(r\"data\\clean_elections.csv\")\n",
    "unemployment=pd.read_csv(r\"data\\unemployment.csv\")\n",
    "gdpcapita=pd.read_csv(r\"data\\gdp_percapita.csv\")\n",
    "population=pd.read_csv(r\"data\\population.csv\")\n",
    "gini_index=pd.read_csv(r\"data\\gini_index.csv\")\n",
    "destination_ref=pd.read_csv(r\"data\\destination_refugees.csv\")\n",
    "origin_ref=pd.read_csv(r\"data\\origin_refugees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49edc2d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>clean_elections</th>\n",
       "      <th>gdp_capita</th>\n",
       "      <th>population</th>\n",
       "      <th>gini_index</th>\n",
       "      <th>outgoing_ref</th>\n",
       "      <th>incomming_ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995</td>\n",
       "      <td>15.400</td>\n",
       "      <td>0.483679</td>\n",
       "      <td>1466.044512</td>\n",
       "      <td>4588842.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>511.0</td>\n",
       "      <td>1329466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.100</td>\n",
       "      <td>0.481211</td>\n",
       "      <td>1463.887967</td>\n",
       "      <td>4732848.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>707.0</td>\n",
       "      <td>1390473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>14.400</td>\n",
       "      <td>0.476509</td>\n",
       "      <td>1494.510627</td>\n",
       "      <td>4848536.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1438102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>13.500</td>\n",
       "      <td>0.473261</td>\n",
       "      <td>1600.397931</td>\n",
       "      <td>4943975.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>897.0</td>\n",
       "      <td>1488225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999</td>\n",
       "      <td>14.380</td>\n",
       "      <td>0.471677</td>\n",
       "      <td>1619.535865</td>\n",
       "      <td>5031754.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>970.0</td>\n",
       "      <td>1542413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>13.706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1651.621798</td>\n",
       "      <td>5122495.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>915.0</td>\n",
       "      <td>1610630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001</td>\n",
       "      <td>14.691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1720.361427</td>\n",
       "      <td>5217328.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>773.0</td>\n",
       "      <td>1663289.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2002</td>\n",
       "      <td>15.325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1802.055064</td>\n",
       "      <td>5317514.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>1699469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2003</td>\n",
       "      <td>14.439</td>\n",
       "      <td>0.472965</td>\n",
       "      <td>1876.259338</td>\n",
       "      <td>5434036.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>1741365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2004</td>\n",
       "      <td>14.700</td>\n",
       "      <td>0.471377</td>\n",
       "      <td>2044.963723</td>\n",
       "      <td>5580241.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1777765.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2005</td>\n",
       "      <td>14.844</td>\n",
       "      <td>0.471107</td>\n",
       "      <td>2183.394643</td>\n",
       "      <td>5765639.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>1828839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2006</td>\n",
       "      <td>14.058</td>\n",
       "      <td>0.471932</td>\n",
       "      <td>2513.028732</td>\n",
       "      <td>5991547.0</td>\n",
       "      <td>33.9</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>2358587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007</td>\n",
       "      <td>12.700</td>\n",
       "      <td>0.465505</td>\n",
       "      <td>2735.378767</td>\n",
       "      <td>6255290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>2403763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2008</td>\n",
       "      <td>12.650</td>\n",
       "      <td>0.465496</td>\n",
       "      <td>3455.769953</td>\n",
       "      <td>6556473.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1883.0</td>\n",
       "      <td>2452009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2009</td>\n",
       "      <td>12.850</td>\n",
       "      <td>0.462905</td>\n",
       "      <td>3559.692102</td>\n",
       "      <td>6893258.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2111.0</td>\n",
       "      <td>2434485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010</td>\n",
       "      <td>12.475</td>\n",
       "      <td>0.483180</td>\n",
       "      <td>3736.645462</td>\n",
       "      <td>7261541.0</td>\n",
       "      <td>33.7</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>2393339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>12.875</td>\n",
       "      <td>0.491616</td>\n",
       "      <td>3852.890025</td>\n",
       "      <td>7662858.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>2430580.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012</td>\n",
       "      <td>12.150</td>\n",
       "      <td>0.489569</td>\n",
       "      <td>3910.346894</td>\n",
       "      <td>8089963.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2363.0</td>\n",
       "      <td>2337341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2013</td>\n",
       "      <td>12.600</td>\n",
       "      <td>0.538746</td>\n",
       "      <td>4044.426869</td>\n",
       "      <td>8518992.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>2712877.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014</td>\n",
       "      <td>11.875</td>\n",
       "      <td>0.536661</td>\n",
       "      <td>4131.447350</td>\n",
       "      <td>8918822.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>2771492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015</td>\n",
       "      <td>13.075</td>\n",
       "      <td>0.536953</td>\n",
       "      <td>4164.108769</td>\n",
       "      <td>9266573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>2808343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016</td>\n",
       "      <td>15.275</td>\n",
       "      <td>0.546012</td>\n",
       "      <td>4175.356602</td>\n",
       "      <td>9554286.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1919.0</td>\n",
       "      <td>2860679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017</td>\n",
       "      <td>18.300</td>\n",
       "      <td>0.550606</td>\n",
       "      <td>4231.518280</td>\n",
       "      <td>9785840.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>2897751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>18.600</td>\n",
       "      <td>0.550047</td>\n",
       "      <td>4308.151074</td>\n",
       "      <td>9965322.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>2957877.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  unemployment  clean_elections   gdp_capita  population  gini_index  \\\n",
       "0   1995        15.400         0.483679  1466.044512   4588842.0         NaN   \n",
       "1   1996        13.100         0.481211  1463.887967   4732848.0         NaN   \n",
       "2   1997        14.400         0.476509  1494.510627   4848536.0        36.4   \n",
       "3   1998        13.500         0.473261  1600.397931   4943975.0         NaN   \n",
       "4   1999        14.380         0.471677  1619.535865   5031754.0         NaN   \n",
       "5   2000        13.706         0.000000  1651.621798   5122495.0         NaN   \n",
       "6   2001        14.691         0.000000  1720.361427   5217328.0         NaN   \n",
       "7   2002        15.325         0.000000  1802.055064   5317514.0        37.0   \n",
       "8   2003        14.439         0.472965  1876.259338   5434036.0         NaN   \n",
       "9   2004        14.700         0.471377  2044.963723   5580241.0         NaN   \n",
       "10  2005        14.844         0.471107  2183.394643   5765639.0         NaN   \n",
       "11  2006        14.058         0.471932  2513.028732   5991547.0        33.9   \n",
       "12  2007        12.700         0.465505  2735.378767   6255290.0         NaN   \n",
       "13  2008        12.650         0.465496  3455.769953   6556473.0        32.6   \n",
       "14  2009        12.850         0.462905  3559.692102   6893258.0         NaN   \n",
       "15  2010        12.475         0.483180  3736.645462   7261541.0        33.7   \n",
       "16  2011        12.875         0.491616  3852.890025   7662858.0         NaN   \n",
       "17  2012        12.150         0.489569  3910.346894   8089963.0         NaN   \n",
       "18  2013        12.600         0.538746  4044.426869   8518992.0         NaN   \n",
       "19  2014        11.875         0.536661  4131.447350   8918822.0         NaN   \n",
       "20  2015        13.075         0.536953  4164.108769   9266573.0         NaN   \n",
       "21  2016        15.275         0.546012  4175.356602   9554286.0         NaN   \n",
       "22  2017        18.300         0.550606  4231.518280   9785840.0         NaN   \n",
       "23  2018        18.600         0.550047  4308.151074   9965322.0         NaN   \n",
       "\n",
       "    outgoing_ref  incomming_ref  \n",
       "0          511.0      1329466.0  \n",
       "1          707.0      1390473.0  \n",
       "2          720.0      1438102.0  \n",
       "3          897.0      1488225.0  \n",
       "4          970.0      1542413.0  \n",
       "5          915.0      1610630.0  \n",
       "6          773.0      1663289.0  \n",
       "7         1280.0      1699469.0  \n",
       "8         1149.0      1741365.0  \n",
       "9         1158.0      1777765.0  \n",
       "10        1778.0      1828839.0  \n",
       "11        1592.0      2358587.0  \n",
       "12        1778.0      2403763.0  \n",
       "13        1883.0      2452009.0  \n",
       "14        2111.0      2434485.0  \n",
       "15        2250.0      2393339.0  \n",
       "16        2244.0      2430580.0  \n",
       "17        2363.0      2337341.0  \n",
       "18        1622.0      2712877.0  \n",
       "19        1711.0      2771492.0  \n",
       "20        1836.0      2808343.0  \n",
       "21        1919.0      2860679.0  \n",
       "22        2119.0      2897751.0  \n",
       "23        2426.0      2957877.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country='JOR'\n",
    "result = unemployment.loc[(unemployment['Country ISO3'] == country) & (unemployment['Indicator Id']==3296), \n",
    "                          pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "result = result.transpose().reset_index()\n",
    "result.columns=['year','unemployment']\n",
    "temp = clean_elections.loc[(clean_elections['Country ISO3'] == country) & (clean_elections['Indicator Id']==41828), \n",
    "                         pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','clean_elections']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "temp=gdpcapita.loc[gdpcapita['Country Code'] == country,\n",
    "                  pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','gdp_capita']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "temp=population.loc[population['Country Code'] == country,\n",
    "                  pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','population']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "temp=gini_index.loc[gini_index['Country Code'] == country,\n",
    "                  pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','gini_index']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "temp=origin_ref.loc[origin_ref['Country Code'] == country,\n",
    "                  pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','outgoing_ref']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "temp=destination_ref.loc[destination_ref['Country Code'] == country,\n",
    "                  pd.Series(range(int('1995'),int('2018')+1)).astype(str)]\n",
    "temp = temp.transpose().reset_index()\n",
    "temp.columns=['year','incomming_ref']\n",
    "result = pd.merge(result,temp,how='left', on='year')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285f6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add NEIGHBORS column\n",
    "countries[\"neighbors\"] = None  \n",
    "\n",
    "for index, ctry in countries.iterrows():   \n",
    "\n",
    "    # get 'not disjoint' countries\n",
    "    neighbors = countries[~countries.geometry.disjoint(ctry.geometry)].ISO_A3.tolist()\n",
    "\n",
    "    # remove own name of the country from the list\n",
    "    neighbors = [ name for name in neighbors if ctry.ISO_A3 != name ]\n",
    "\n",
    "    # add names of neighbors as NEIGHBORS value\n",
    "    countries.at[index, \"neighbors\"] = \", \".join(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb4195c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighboring_out_ref=origin_ref.loc[origin_ref['Country Code'].isin(\n",
    "    [x.strip(' ') for x in countries[countries.ISO_A3 == country].neighbors.str.split(',').reset_index(drop=True)[0]]),\n",
    "                                   pd.Series(range(int('1995'),int('2018')+1)).astype(str)].sum()\n",
    "result=result.assign(outgoing_ref_neighbors = neighboring_out_ref.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6739fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['year'] = pd.to_datetime(result.year , format = '%Y')\n",
    "data = result.drop(['year'], axis=1)\n",
    "data.index = result.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6aba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing value treatment\n",
    "cols = data.columns\n",
    "for j in cols:\n",
    "    for i in range(0,len(data)):\n",
    "        if pd.isna(data[j][i]):\n",
    "            data[j][i] = data[j][i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca49dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['incomming_ref','outgoing_ref','population','gdp_capita',\n",
    "           'clean_elections','unemployment','outgoing_ref_neighbors']]#gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bea741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 7)\n",
      "(5, 7)\n"
     ]
    }
   ],
   "source": [
    "test_split=round(len(df)*0.20)\n",
    "df_for_training=df[:-test_split]\n",
    "df_for_testing=df[-test_split:]\n",
    "print(df_for_training.shape)\n",
    "print(df_for_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f34f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_for_training_scaled = scaler.fit_transform(df_for_training)\n",
    "df_for_testing_scaled=scaler.transform(df_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a693aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,0])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "\n",
    "trainX,trainY=createXY(df_for_training_scaled,4)\n",
    "testX,testY=createXY(df_for_testing_scaled,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "146e3ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ioliv\\AppData\\Local\\Temp\\ipykernel_42176\\2463399500.py:11: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  grid_model = KerasRegressor(build_fn=build_model,verbose=1,validation_data=(testX,testY))\n"
     ]
    }
   ],
   "source": [
    "def build_model(optimizer):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(50,return_sequences=True,input_shape=(4,7)))\n",
    "    grid_model.add(LSTM(50))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1))\n",
    "    \n",
    "    grid_model.compile(loss = 'mse',optimizer = optimizer)\n",
    "    return grid_model\n",
    "\n",
    "grid_model = KerasRegressor(build_fn=build_model,verbose=1,validation_data=(testX,testY))\n",
    "\n",
    "parameters = {'batch_size' : [16,20],\n",
    "              'epochs' : [8,10],\n",
    "              'optimizer' : ['adam','Adadelta'] }\n",
    "\n",
    "grid_search  = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43c6597",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6866 - val_loss: 1.2197\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5844 - val_loss: 1.0901\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5715 - val_loss: 0.9683\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4639 - val_loss: 0.8539\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4115 - val_loss: 0.7447\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3924 - val_loss: 0.6406\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.3066 - val_loss: 0.5409\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2556 - val_loss: 0.4457\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0344\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1408 - val_loss: 1.2826\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1239 - val_loss: 1.1947\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1090 - val_loss: 1.1081\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0957 - val_loss: 1.0226\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0825 - val_loss: 0.9374\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0756 - val_loss: 0.8530\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0630 - val_loss: 0.7685\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0485 - val_loss: 0.6837\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3294\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6333 - val_loss: 1.2261\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6574 - val_loss: 1.2259\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6465 - val_loss: 1.2257\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6707 - val_loss: 1.2255\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6209 - val_loss: 1.2253\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6347 - val_loss: 1.2250\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6460 - val_loss: 1.2248\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6305 - val_loss: 1.2246\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1201\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1518 - val_loss: 1.2542\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1480 - val_loss: 1.2540\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1460 - val_loss: 1.2539\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1483 - val_loss: 1.2538\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1467 - val_loss: 1.2537\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1461 - val_loss: 1.2536\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1550 - val_loss: 1.2535\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1414 - val_loss: 1.2534\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6523\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.5427 - val_loss: 0.9023\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4669 - val_loss: 0.7849\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4044 - val_loss: 0.6742\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3578 - val_loss: 0.5692\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3123 - val_loss: 0.4696\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2548 - val_loss: 0.3756\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2147 - val_loss: 0.2878\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1763 - val_loss: 0.2074\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1215 - val_loss: 0.1364\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0795 - val_loss: 0.0772\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0239\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.1818 - val_loss: 1.4888\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1608 - val_loss: 1.4182\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1297 - val_loss: 1.3525\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1193 - val_loss: 1.2889\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1024 - val_loss: 1.2270\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0785 - val_loss: 1.1660\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0647 - val_loss: 1.1055\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0589 - val_loss: 1.0443\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0442 - val_loss: 0.9826\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0472 - val_loss: 0.9199\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3381\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7882 - val_loss: 1.5953\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7911 - val_loss: 1.5951\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7791 - val_loss: 1.5949\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7504 - val_loss: 1.5948\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.8015 - val_loss: 1.5946\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7758 - val_loss: 1.5944\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7756 - val_loss: 1.5943\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7763 - val_loss: 1.5941\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7640 - val_loss: 1.5939\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7393 - val_loss: 1.5938\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1635\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1162 - val_loss: 1.7449\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1169 - val_loss: 1.7448\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1176 - val_loss: 1.7447\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1152 - val_loss: 1.7446\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1173 - val_loss: 1.7445\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1169 - val_loss: 1.7444\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1208 - val_loss: 1.7442\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1170 - val_loss: 1.7441\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1213 - val_loss: 1.7440\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1174 - val_loss: 1.7439\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7552\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.5978 - val_loss: 1.0890\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5342 - val_loss: 0.9498\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4599 - val_loss: 0.8175\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4121 - val_loss: 0.6917\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3085 - val_loss: 0.5721\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2677 - val_loss: 0.4590\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2156 - val_loss: 0.3534\n",
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1658 - val_loss: 0.2567\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0316\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1336 - val_loss: 1.5230\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1188 - val_loss: 1.4262\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1050 - val_loss: 1.3329\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0935 - val_loss: 1.2421\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0697 - val_loss: 1.1528\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0679 - val_loss: 1.0648\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0564 - val_loss: 0.9782\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0482 - val_loss: 0.8923\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3365\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7630 - val_loss: 1.7069\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7651 - val_loss: 1.7067\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7718 - val_loss: 1.7064\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8054 - val_loss: 1.7062\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7670 - val_loss: 1.7060\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.7721 - val_loss: 1.7057\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7658 - val_loss: 1.7055\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.7946 - val_loss: 1.7053\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1745\n",
      "Epoch 1/8\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1417 - val_loss: 1.4253\n",
      "Epoch 2/8\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1435 - val_loss: 1.4252\n",
      "Epoch 3/8\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1365 - val_loss: 1.4251\n",
      "Epoch 4/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1401 - val_loss: 1.4250\n",
      "Epoch 5/8\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1444 - val_loss: 1.4249\n",
      "Epoch 6/8\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1331 - val_loss: 1.4247\n",
      "Epoch 7/8\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1451 - val_loss: 1.4246\n",
      "Epoch 8/8\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1356 - val_loss: 1.4245\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6940\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.5333 - val_loss: 1.1349\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4837 - val_loss: 1.0024\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4112 - val_loss: 0.8745\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3735 - val_loss: 0.7500\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3034 - val_loss: 0.6289\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2657 - val_loss: 0.5120\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2064 - val_loss: 0.4008\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1641 - val_loss: 0.2969\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1293 - val_loss: 0.2026\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0879 - val_loss: 0.1215\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0186\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.1276 - val_loss: 1.2251\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1151 - val_loss: 1.1081\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0989 - val_loss: 0.9948\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0807 - val_loss: 0.8869\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0667 - val_loss: 0.7810\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0624 - val_loss: 0.6804\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0422 - val_loss: 0.5848\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0343 - val_loss: 0.4931\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0327 - val_loss: 0.4088\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0212 - val_loss: 0.3330\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1466\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.6673 - val_loss: 1.4130\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6566 - val_loss: 1.4128\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6338 - val_loss: 1.4125\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6601 - val_loss: 1.4123\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6522 - val_loss: 1.4121\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6633 - val_loss: 1.4119\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6557 - val_loss: 1.4117\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6686 - val_loss: 1.4115\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6656 - val_loss: 1.4113\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6659 - val_loss: 1.4110\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1417\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1064 - val_loss: 1.1356\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1088 - val_loss: 1.1356\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1077 - val_loss: 1.1355\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1063 - val_loss: 1.1354\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1096 - val_loss: 1.1353\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1034 - val_loss: 1.1352\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1051 - val_loss: 1.1351\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1057 - val_loss: 1.1350\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1087 - val_loss: 1.1349\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1036 - val_loss: 1.1348\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5431\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.4127 - val_loss: 1.5285\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.3790 - val_loss: 1.3756\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3495 - val_loss: 1.2319\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3081 - val_loss: 1.0954\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2795 - val_loss: 0.9644\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2480 - val_loss: 0.8381\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2104 - val_loss: 0.7163\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1803 - val_loss: 0.5989\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1415 - val_loss: 0.4862\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1431 - val_loss: 0.3794\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95e0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_search.best_params_\n",
    "my_model=grid_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68fdc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=my_model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c3149f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_copies_array = np.repeat(prediction,7, axis=-1)\n",
    "pred=scaler.inverse_transform(np.reshape(prediction_copies_array,(len(prediction),7)))[:,0]\n",
    "original_copies_array = np.repeat(testY,7, axis=-1)\n",
    "original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),7)))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c3fe345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Values--  [2105710.2]\n",
      "\n",
      "Original Values--  [2957877.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred Values-- \" ,pred)\n",
    "print(\"\\nOriginal Values-- \" ,original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c87ecffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([852166.75])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original-pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2bde4b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(original, color = 'red', label = 'Real incomming refugees')\n",
    "plt.plot(pred, color = 'blue', label = 'Predicted incomming refugees')\n",
    "plt.title('Incomming refugees')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Refugees')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
